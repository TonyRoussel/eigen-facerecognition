{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import misc\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Householder\n",
    "def make_householder(a):\n",
    "    u = a + np.copysign(np.linalg.norm(a), a[0])\n",
    "    v = a / u[0]\n",
    "    v[0] = 1\n",
    "    H = np.eye(a.shape[0])\n",
    "    beta = 2 / (np.dot(v, v.transpose()))\n",
    "    vtv = np.dot(np.matrix(v).transpose(), np.matrix(v))\n",
    "    H -= np.dot(beta, vtv)\n",
    "    return H\n",
    "\n",
    "def qrDecomposition(A):\n",
    "    m, n = A.shape\n",
    "    Q = np.eye(m)\n",
    "    for i in range(n - (m == n)):\n",
    "        H = np.eye(m)\n",
    "        H[i:, i:] = make_householder(A[i:, i])\n",
    "        Q = np.dot(Q, H)\n",
    "        A = np.dot(H, A)\n",
    "    return Q, A\n",
    "\n",
    "def qr(toCompute, maxIter = 100):\n",
    "    A = []\n",
    "    Q = np.eye(toCompute.shape[0])\n",
    "    A.append(None)\n",
    "    A.append(toCompute)\n",
    "    for k in range(maxIter):\n",
    "        A[0] = A[1]\n",
    "        q, R = qrDecomposition(A[0])\n",
    "        A[1] = np.dot(R, q)\n",
    "        Q = Q.dot(q)\n",
    "    return np.diagonal(A[1]), Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def concatMatrix(mtxLst):\n",
    "    mtx = np.array(())\n",
    "    flatLst = []\n",
    "    for m in mtxLst:\n",
    "        flatLst.append(m.flatten())\n",
    "    mtx = np.vstack(flatLst)\n",
    "    return mtx.transpose()\n",
    "\n",
    "def extractEigenvecOnVal(eigval, eigvec, threshold = 1):\n",
    "    delIdx = np.where(eigval < threshold)[0]\n",
    "    return np.delete(eigvec, delIdx, axis=1)\n",
    "\n",
    "def reconstructVector(M, eigvec):\n",
    "    eigvecT = eigvec.transpose()\n",
    "    szeNewM = (np.shape(eigvec)[1], np.shape(M)[0])\n",
    "    newmatrix = np.empty(szeNewM)\n",
    "    for idx, vec in enumerate(eigvecT):\n",
    "        newvec = np.dot(M, vec.transpose())\n",
    "        newmatrix[idx] = newvec\n",
    "    return newmatrix.transpose()\n",
    "\n",
    "def computeCostMulti(X, y, theta):\n",
    "    H = np.dot(X, theta)\n",
    "    diff = H.transpose() - y\n",
    "    diff = np.power(diff, 2)\n",
    "    sdiff = np.sum(diff, axis=1)\n",
    "    cost = sdiff / (2. * (np.shape(y)[0]))\n",
    "    return cost.item(0)\n",
    "\n",
    "def gradDescent(X, y, theta, alpha, numIter = None):\n",
    "    if numIter is None:\n",
    "        return gradDescentConvergence(X, y, theta, alpha)\n",
    "    return gradDescentIteration(X, y, theta, alpha, numIter)\n",
    "\n",
    "def gradDescentIteration(X, y, theta, alpha, numIter):\n",
    "    m = np.shape(y)[0]\n",
    "    for i in range(numIter):\n",
    "        H = np.dot(X, theta)\n",
    "        diff = H.transpose() - y\n",
    "        sigma = np.dot(X.transpose(), diff.transpose()) / m\n",
    "        theta = theta - alpha * sigma\n",
    "    print \"Last Iteration Cost: \", computeCostMulti(X, y, theta)\n",
    "    return theta\n",
    "\n",
    "def gradDescentConvergence(X, y, theta, alpha):\n",
    "    m = np.shape(y)[0]\n",
    "    i = 0\n",
    "    cost = computeCostMulti(X, y, theta)\n",
    "    costp = cost + 1\n",
    "    diff = costp - cost\n",
    "    while (diff > 1e-100):\n",
    "        H = np.dot(X, theta)\n",
    "        diff = H.transpose() - y\n",
    "        sigma = np.dot(X.transpose(), diff.transpose()) / m\n",
    "        theta = theta - alpha * sigma\n",
    "        costp = cost\n",
    "        cost = computeCostMulti(X, y, theta)\n",
    "        diff = costp - cost\n",
    "        i = i + 1\n",
    "    print \"Convergence Cost (\", i + 1,  \"iteration ): \", computeCostMulti(X, y, theta)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learRate = 1e-11\n",
    "maxIteration = 300\n",
    "img_extension = \".pgm\"\n",
    "trainPath = \"../faceset/sample/train/\"\n",
    "validPath = \"../faceset/sample/valid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "usleep = lambda x: time.sleep(x/1000000.0)\n",
    "\n",
    "def loadmatrixs(path):\n",
    "    matrixs = []\n",
    "    sze = len(os.listdir(path))\n",
    "    for i, filename in enumerate(os.listdir(path)):\n",
    "        if not filename.endswith(img_extension):\n",
    "            continue\n",
    "        img = misc.imread(path + filename)\n",
    "        matrixs.append((filename, img))\n",
    "        usleep(250)\n",
    "    return matrixs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(mtxLst):\n",
    "    thetas = list()\n",
    "    M = concatMatrix(mtxLst)\n",
    "    Mmean = M.mean(axis=1)\n",
    "    M -= Mmean[:, np.newaxis]\n",
    "    Mtld = np.dot(M.transpose(), M)\n",
    "    n = np.shape(Mtld)[1]\n",
    "    #eigenval, eigenvec = qr(Mtld, 400)\n",
    "    eigenval, eigenvec = np.linalg.eig(Mtld)\n",
    "    eigenvec = extractEigenvecOnVal(eigenval, eigenvec, 1)\n",
    "    eigenvec = reconstructVector(M, eigenvec)\n",
    "    print \"eigenvec shape: \", np.shape(eigenvec)\n",
    "    for i in range(n):\n",
    "        img = M.transpose()[i]\n",
    "        theta = np.ones(np.shape(eigenvec)[1])\n",
    "        theta = gradDescent(eigenvec, np.matrix(img), np.matrix(theta).transpose(), learRate, maxIteration)\n",
    "        print \"Descent terminated: \", i, \" / \", n - 1  ######\n",
    "        thetas.append(theta)\n",
    "    return (Mmean, eigenvec, thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submit(mtx, mean, eigenvec):\n",
    "    mtxflat = mtx.flatten()\n",
    "    mtxflat = np.vstack(list(mtxflat))\n",
    "    mtxflat -= mean[:, np.newaxis]\n",
    "    mtxflat = mtxflat.transpose()[0]\n",
    "    theta = np.ones(np.shape(eigenvec)[1])\n",
    "    theta = gradDescent(eigenvec, np.matrix(mtxflat), np.matrix(theta).transpose(), learRate, maxIteration)\n",
    "    return theta\n",
    "\n",
    "def compare(thetaSubmit, thetas):\n",
    "    minIdx = 0\n",
    "    minVal = np.absolute(np.sum(thetas[0] - thetaSubmit))\n",
    "    for idx, theta in enumerate(thetas):\n",
    "        val = np.absolute(np.sum(np.absolute(theta - thetaSubmit)))\n",
    "        if val < minVal:\n",
    "            minIdx = idx\n",
    "            minVal = val\n",
    "    return minIdx\n",
    "\n",
    "def compareAvgGap(thetaSubmit, thetas):\n",
    "    minIdx = 0\n",
    "    minVal = (np.absolute(thetas[0] - thetaSubmit)).mean()\n",
    "    for idx, theta in enumerate(thetas):\n",
    "        val = (np.absolute(theta - thetaSubmit)).mean()\n",
    "        if val < minVal:\n",
    "            minIdx = idx\n",
    "            minVal = val\n",
    "    return minIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainD = loadmatrixs(trainPath)\n",
    "validD = loadmatrixs(validPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ftrainD, mtrainD = zip(*trainD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eigenvec shape:  (10304, 36)\n",
      "Last Iteration Cost:  760387406.921\n",
      "Descent terminated:  0  /  71\n",
      "Last Iteration Cost:  837783246.123\n",
      "Descent terminated:  1  /  71\n",
      "Last Iteration Cost:  781212673.066\n",
      "Descent terminated:  2  /  71\n",
      "Last Iteration Cost:  708679080.248\n",
      "Descent terminated:  3  /  71\n",
      "Last Iteration Cost:  766684879.835\n",
      "Descent terminated:  4  /  71\n",
      "Last Iteration Cost:  753194125.107\n",
      "Descent terminated:  5  /  71\n",
      "Last Iteration Cost:  770338979.758\n",
      "Descent terminated:  6  /  71\n",
      "Last Iteration Cost:  726070636.568\n",
      "Descent terminated:  7  /  71\n",
      "Last Iteration Cost:  821827296.558\n",
      "Descent terminated:  8  /  71\n",
      "Last Iteration Cost:  816981034.661\n",
      "Descent terminated:  9  /  71\n",
      "Last Iteration Cost:  800618041.514\n",
      "Descent terminated:  10  /  71\n",
      "Last Iteration Cost:  813400052.595\n",
      "Descent terminated:  11  /  71\n",
      "Last Iteration Cost:  849993738.169\n",
      "Descent terminated:  12  /  71\n",
      "Last Iteration Cost:  869093757.78\n",
      "Descent terminated:  13  /  71\n",
      "Last Iteration Cost:  843223670.624\n",
      "Descent terminated:  14  /  71\n",
      "Last Iteration Cost:  818334429.417\n",
      "Descent terminated:  15  /  71\n",
      "Last Iteration Cost:  816365977.085\n",
      "Descent terminated:  16  /  71\n",
      "Last Iteration Cost:  813283212.971\n",
      "Descent terminated:  17  /  71\n",
      "Last Iteration Cost:  801667680.782\n",
      "Descent terminated:  18  /  71\n",
      "Last Iteration Cost:  806119184.37\n",
      "Descent terminated:  19  /  71\n",
      "Last Iteration Cost:  839426445.797\n",
      "Descent terminated:  20  /  71\n",
      "Last Iteration Cost:  804989323.304\n",
      "Descent terminated:  21  /  71\n",
      "Last Iteration Cost:  776153602.004\n",
      "Descent terminated:  22  /  71\n",
      "Last Iteration Cost:  814659311.095\n",
      "Descent terminated:  23  /  71\n",
      "Last Iteration Cost:  791096280.015\n",
      "Descent terminated:  24  /  71\n",
      "Last Iteration Cost:  772281454.897\n",
      "Descent terminated:  25  /  71\n",
      "Last Iteration Cost:  793779085.93\n",
      "Descent terminated:  26  /  71\n",
      "Last Iteration Cost:  729908176.776\n",
      "Descent terminated:  27  /  71\n",
      "Last Iteration Cost:  832696193.763\n",
      "Descent terminated:  28  /  71\n",
      "Last Iteration Cost:  841296268.545\n",
      "Descent terminated:  29  /  71\n",
      "Last Iteration Cost:  799736030.869\n",
      "Descent terminated:  30  /  71\n",
      "Last Iteration Cost:  802543262.714\n",
      "Descent terminated:  31  /  71\n",
      "Last Iteration Cost:  817380581.01\n",
      "Descent terminated:  32  /  71\n",
      "Last Iteration Cost:  754198400.397\n",
      "Descent terminated:  33  /  71\n",
      "Last Iteration Cost:  835758617.964\n",
      "Descent terminated:  34  /  71\n",
      "Last Iteration Cost:  789103713.025\n",
      "Descent terminated:  35  /  71\n",
      "Last Iteration Cost:  768412904.928\n",
      "Descent terminated:  36  /  71\n",
      "Last Iteration Cost:  851695133.095\n",
      "Descent terminated:  37  /  71\n",
      "Last Iteration Cost:  778046263.146\n",
      "Descent terminated:  38  /  71\n",
      "Last Iteration Cost:  775187559.58\n",
      "Descent terminated:  39  /  71\n",
      "Last Iteration Cost:  824318921.689\n",
      "Descent terminated:  40  /  71\n",
      "Last Iteration Cost:  824805961.386\n",
      "Descent terminated:  41  /  71\n",
      "Last Iteration Cost:  771674047.268\n",
      "Descent terminated:  42  /  71\n",
      "Last Iteration Cost:  791848271.751\n",
      "Descent terminated:  43  /  71\n",
      "Last Iteration Cost:  777606663.119\n",
      "Descent terminated:  44  /  71\n",
      "Last Iteration Cost:  788917996.391\n",
      "Descent terminated:  45  /  71\n",
      "Last Iteration Cost:  787516663.253\n",
      "Descent terminated:  46  /  71\n",
      "Last Iteration Cost:  735113582.111\n",
      "Descent terminated:  47  /  71\n",
      "Last Iteration Cost:  811913085.745\n",
      "Descent terminated:  48  /  71\n",
      "Last Iteration Cost:  854562625.375\n",
      "Descent terminated:  49  /  71\n",
      "Last Iteration Cost:  867906468.335\n",
      "Descent terminated:  50  /  71\n",
      "Last Iteration Cost:  775087078.813\n",
      "Descent terminated:  51  /  71\n",
      "Last Iteration Cost:  876697248.892\n",
      "Descent terminated:  52  /  71\n",
      "Last Iteration Cost:  836025506.063\n",
      "Descent terminated:  53  /  71\n",
      "Last Iteration Cost:  824981223.918\n",
      "Descent terminated:  54  /  71\n",
      "Last Iteration Cost:  808944381.706\n",
      "Descent terminated:  55  /  71\n",
      "Last Iteration Cost:  873198422.024\n",
      "Descent terminated:  56  /  71\n",
      "Last Iteration Cost:  843472630.763\n",
      "Descent terminated:  57  /  71\n",
      "Last Iteration Cost:  832879891.987\n",
      "Descent terminated:  58  /  71\n",
      "Last Iteration Cost:  862485727.043\n",
      "Descent terminated:  59  /  71\n",
      "Last Iteration Cost:  863466494.525\n",
      "Descent terminated:  60  /  71\n",
      "Last Iteration Cost:  860339924.238\n",
      "Descent terminated:  61  /  71\n",
      "Last Iteration Cost:  827117459.587\n",
      "Descent terminated:  62  /  71\n",
      "Last Iteration Cost:  805633684.93\n",
      "Descent terminated:  63  /  71\n",
      "Last Iteration Cost:  726491531.982\n",
      "Descent terminated:  64  /  71\n",
      "Last Iteration Cost:  744740544.422\n",
      "Descent terminated:  65  /  71\n",
      "Last Iteration Cost:  826839049.008\n",
      "Descent terminated:  66  /  71\n",
      "Last Iteration Cost:  760735825.312\n",
      "Descent terminated:  67  /  71\n",
      "Last Iteration Cost:  780288058.472\n",
      "Descent terminated:  68  /  71\n",
      "Last Iteration Cost:  802839306.533\n",
      "Descent terminated:  69  /  71\n",
      "Last Iteration Cost:  786791359.904\n",
      "Descent terminated:  70  /  71\n",
      "Last Iteration Cost:  809654446.827\n",
      "Descent terminated:  71  /  71\n"
     ]
    }
   ],
   "source": [
    "mean, eigenvec, thetas = train(mtrainD)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:  [ 96.88888889  97.19444444  97.19444444 ...,  69.55555556  68.52777778\n",
      "  68.90277778]\n",
      "eigenvec: \n",
      "[[  717.75543729   -13.63753755   -75.11343491 ...,    87.03227508\n",
      "    -42.13666284    95.50391127]\n",
      " [  712.02391005   -12.85908678   -80.59735887 ...,    90.98844297\n",
      "    -32.57341363    99.22794992]\n",
      " [  711.70314966   -11.648162     -69.88430745 ...,    98.57303777\n",
      "    -29.08482165    94.70800891]\n",
      " ..., \n",
      " [ 1139.8864403    -87.52383332   -42.57667544 ...,    76.71106459\n",
      "    -75.55834238   196.68810889]\n",
      " [ 1169.70202924  -134.05159394   -93.58776596 ...,    85.28502621\n",
      "    -50.61688637   156.10907307]\n",
      " [ 1142.04912069   -89.26493667   -40.97930376 ...,    89.68394148\n",
      "    -64.9836286    198.36542548]]\n"
     ]
    }
   ],
   "source": [
    "print \"Mean: \", mean\n",
    "print \"eigenvec: \\n\", eigenvec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Iteration Cost:  812776546.343\n",
      "s1_0001.pgm  -->  s1_0004.pgm [X]\n",
      "Last Iteration Cost:  780695337.625\n",
      "s1_0002.pgm  -->  s1_0009.pgm [X]\n",
      "Last Iteration Cost:  812870774.773\n",
      "s2_0001.pgm  -->  s2_0004.pgm [X]\n",
      "Last Iteration Cost:  845568993.761\n",
      "s2_0002.pgm  -->  s2_0008.pgm [X]\n",
      "Last Iteration Cost:  823057006.294\n",
      "s3_0001.pgm  -->  s3_0004.pgm [X]\n",
      "Last Iteration Cost:  809922519.276\n",
      "s3_0002.pgm  -->  s3_0010.pgm [X]\n",
      "Last Iteration Cost:  818346092.62\n",
      "s4_0001.pgm  -->  s4_0004.pgm [X]\n",
      "Last Iteration Cost:  773140436.63\n",
      "s4_0002.pgm  -->  s4_0006.pgm [X]\n",
      "Last Iteration Cost:  830363649.366\n",
      "s5_0001.pgm  -->  s5_0005.pgm [X]\n",
      "Last Iteration Cost:  814126588.046\n",
      "s5_0002.pgm  -->  s5_0010.pgm [X]\n",
      "Last Iteration Cost:  808028412.626\n",
      "s6_0001.pgm  -->  s6_0005.pgm [X]\n",
      "Last Iteration Cost:  779350461.56\n",
      "s6_0002.pgm  -->  s6_0005.pgm [X]\n",
      "Last Iteration Cost:  835108190.163\n",
      "s7_0001.pgm  -->  s7_0004.pgm [X]\n",
      "Last Iteration Cost:  832697502.113\n",
      "s7_0002.pgm  -->  s7_0005.pgm [X]\n",
      "Last Iteration Cost:  853592678.375\n",
      "s8_0001.pgm  -->  s8_0003.pgm [X]\n",
      "Last Iteration Cost:  828346883.233\n",
      "s8_0002.pgm  -->  s8_0004.pgm [X]\n",
      "Last Iteration Cost:  771798709.747\n",
      "s9_0001.pgm  -->  s9_0004.pgm [X]\n",
      "Last Iteration Cost:  784839069.778\n",
      "s9_0002.pgm  -->  s9_0006.pgm [X]\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(validD):\n",
    "    success = False\n",
    "    filename, mtx = data\n",
    "    thetaSubmit = submit(mtx, mean, eigenvec)\n",
    "    matchIdx = compareAvgGap(thetaSubmit, thetas)\n",
    "    if filename[:filename.rfind(\"_\")] == ftrainD[matchIdx][:ftrainD[matchIdx].rfind(\"_\")]:\n",
    "        success = True\n",
    "        count = count + 1\n",
    "    if success is True:\n",
    "        print filename, \" --> \", ftrainD[matchIdx], \"[X]\"\n",
    "    else:\n",
    "        print filename, \" --> \", ftrainD[matchIdx], \"[ ]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18  /  18 ===> 100.0 %\n"
     ]
    }
   ],
   "source": [
    "print count, \" / \", idx + 1, \"===>\", count / (idx + 1.) * 100, \"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
